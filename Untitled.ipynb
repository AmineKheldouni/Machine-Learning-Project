{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning from the crowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Modules importés\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import *\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bloc génération de données:\n",
    "def gen_arti(centerx=1,centery=1,sigma=0.1,nbex=1000,data_type=0,epsilon=0.02):\n",
    "    \"\"\" Generateur de donnees,\n",
    "        :param centerx: centre des gaussiennes\n",
    "        :param centery:\n",
    "        :param sigma: des gaussiennes\n",
    "        :param nbex: nombre d'exemples\n",
    "        :param data_type: 0: melange 2 gaussiennes, 1: melange 4 gaussiennes, 2:echequier\n",
    "        :param epsilon: bruit dans les donnees\n",
    "        :return: data matrice 2d des donnnes,y etiquette des donnnees\n",
    "    \"\"\"\n",
    "    if data_type==0:\n",
    "         #melange de 2 gaussiennes\n",
    "         xpos=np.random.multivariate_normal([centerx,centerx],np.diag([sigma,sigma]),int(nbex//2))\n",
    "         xneg=np.random.multivariate_normal([-centerx,-centerx],np.diag([sigma,sigma]),int(nbex//2))\n",
    "         data=np.vstack((xpos,xneg))\n",
    "         y=np.hstack((np.ones(nbex//2),-np.ones(nbex//2)))\n",
    "    if data_type==1:\n",
    "        #melange de 4 gaussiennes\n",
    "        xpos=np.vstack((np.random.multivariate_normal([centerx,centerx],np.diag([sigma,sigma]),int(nbex//4)),np.random.multivariate_normal([-centerx,-centerx],np.diag([sigma,sigma]),int(nbex/4))))\n",
    "        xneg=np.vstack((np.random.multivariate_normal([-centerx,centerx],np.diag([sigma,sigma]),int(nbex//4)),np.random.multivariate_normal([centerx,-centerx],np.diag([sigma,sigma]),int(nbex/4))))\n",
    "        data=np.vstack((xpos,xneg))\n",
    "        y=np.hstack((np.ones(nbex//2),-np.ones(int(nbex//2))))\n",
    "\n",
    "    if data_type==2:\n",
    "        #echiquier\n",
    "        data=np.reshape(np.random.uniform(-4,4,2*nbex),(nbex,2))\n",
    "        y=np.ceil(data[:,0])+np.ceil(data[:,1])\n",
    "        y=2*(y % 2)-1\n",
    "    # un peu de bruit\n",
    "    data[:,0]+=np.random.normal(0,epsilon,nbex)\n",
    "    data[:,1]+=np.random.normal(0,epsilon,nbex)\n",
    "    # on mélange les données\n",
    "    idx = np.random.permutation((range(y.size)))\n",
    "    data=data[idx,:]\n",
    "    y=y[idx]\n",
    "    return data,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1.]\n",
      " [ 1.  1.  1.]\n",
      " [-1. -1. -1.]\n",
      " [ 1.  1.  1.]]\n",
      "[-1.  1. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Fonction de générations de données \n",
    "\n",
    "#Proba que l'annotateur est raison\n",
    "\n",
    "liste_annotateur_bernouilli=[0.9,0.9,0.9]\n",
    "\n",
    "\n",
    "def fonction_modification_annotateur(label,proba):\n",
    "    valeur_proba=np.random.uniform(0,1)\n",
    "    label_res=label\n",
    "    if(valeur_proba>=proba):\n",
    "        label_res=-label\n",
    "        \n",
    "    return label_res\n",
    "        \n",
    "        \n",
    "\n",
    "def generation_bernouilli(nombre_exemple,datatype):\n",
    "    xtrain,ytrain = gen_arti(nbex=nombre_exemple,data_type=0,epsilon=0.2)\n",
    "    #changement des labels \n",
    "    y_annote=np.zeros((nombre_exemple,len(liste_annotateur_bernouilli)))\n",
    "    \n",
    "    for i in range(len(liste_annotateur_bernouilli)):\n",
    "        fonction_annotation=lambda x:fonction_modification_annotateur(x,liste_annotateur_bernouilli[i])\n",
    "        fonction_annotation=np.vectorize(fonction_annotation)\n",
    "        y_annote[:,i]=fonction_annotation(ytrain)\n",
    "    return xtrain,y_annote,ytrain\n",
    "\n",
    "        \n",
    "xtrain,y_annote,ytrain=generation_bernouilli(4,0)\n",
    "print(y_annote)\n",
    "print(ytrain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LearnCrowd:\n",
    "    def __init__(self, T, N, d):\n",
    "        self.alpha = np.zeros((1,d)) # Poids des dimensions\n",
    "        self.beta = 0\n",
    "        self.w = np.zeros((d,T)) # Poids des labelleurs\n",
    "        self.gamma = np.zeros((1,T))\n",
    "    \n",
    "    def rlog(self, i,t,X,w,gamma):\n",
    "        return 1/(1+np.exp(-np.dot(w[:,t].T,X[i,:])-gamma[1,t]))  #indice de la donnée, indice du labelleur\n",
    "    \n",
    "    rlog=np.vectorize(rlog)\n",
    "    \n",
    "    def z_cond_x(self, X, alpha, beta):  \n",
    "        #proba que le label soit 0 ou 1 sachant la donnée (Rlog)\n",
    "        z_cond_x = np.zeros((X.shape[0],2))\n",
    "        sigm = lambda x:  1/(1+np.exp(-x))\n",
    "        sigm=np.vectorize(sigm)\n",
    "        tmpsigm = sigm(-np.dot(X,alpha.T)-beta)\n",
    "        z_cond_x[:,0] = list(tmpsigm)\n",
    "        z_cond_x[:,1] = list(1-np.array(z_cond_x[:,0]))\n",
    "        return z_cond_x\n",
    "    \n",
    "    def likelihoodBernoulli(self, X, Y, alpha, beta, gamma, w):\n",
    "        N = X.shape[0]\n",
    "        T = Y.shape[1]\n",
    "        #proba cond du label Yt du labelleur t pour la donnée i sachant le vrai label 0 ou 1 (Bernouilli)\n",
    "        y_cond_z = np.zeros((N,T,2))\n",
    "        for t in range(T):\n",
    "            y_cond_z[:,t,0] = pow(1-self.rlog(np.arange(1,X.shape[0]+1),t,X,w,gamma),np.abs(Y[:,t]))\\\n",
    "            *pow(self.rlog(np.arange(1,X.shape[0]+1),t,X,w,gamma),1-np.abs(Y[:,t]))\n",
    "            y_cond_z[:,t,1] = pow(1-self.rlog(np.arange(1,X.shape[0]+1),t,X,w,gamma),np.abs(Y[:,t]-1))\\\n",
    "            *pow(self.rlog(np.arange(1,X.shape[0]+1),t,X,w,gamma),1-np.abs(Y[:,t]-1))\n",
    "            # IL EST OU ABS(Y_i - Z_i) ???\n",
    "        #hyp de base que l'on pourra prendre pour simplifier neta[i,t]=rlog(i,t)=neta[t]\n",
    "        #cet hyp revient à donner une proba constante de se tromper pour le labelleur t quelque soit la donnée\n",
    "        #il faudrait alors rajouter un self.neta=np.zeros(1,T) au init pour le modèle de Bernouilli\n",
    "        \n",
    "        #proba que le label soit 0 ou 1 sachant la donnée (Rlog)\n",
    "        #z_cond_x(i,1) = 1/(1+exp(-np.dot(self.alpha.T,X(i,:))-self.beta))\n",
    "        #z_cond_x(i,2) = 1-z_cond_x(i,1)\n",
    "        \n",
    "        return np.multiply(np.prod(y_cond_z,axis=1),self.z_cond_x(X, alpha, beta))\n",
    "    \n",
    "    \n",
    "    def likelihoodGaussian(self, X, Y, alpha, beta, gamma, w):\n",
    "        #proba cond du label Yt du labelleur t pour la donnée i sachant le vrai label 0 ou 1 (Bernouilli)\n",
    "        y_cond_z_cond_x = np.zeros((N,T,2))\n",
    "        norm=lambda x,mu,sigma:1/(sqrt(2*np.pi)*sigma)*np.exp(-pow((x-mu),2)/pow(sigma,2))\n",
    "        norm=np.vectorize(norm)\n",
    "        for t in range(T):\n",
    "            y_cond_z_cond_x[:,t,0] = norm(Y[:,t],0,self.rlog(np.arange(1,X.shape[0]+1),t,X,w,gamma))\n",
    "            y_cond_z_cond_x[:,t,1] = norm(Y[:,t],1,self.rlog(np.arange(1,X.shape[0]+1),t,X,w,gamma))\n",
    "        return np.multiply(np.prod(y_cond_z_cond_x,axis=1),self.z_cond_x(X, alpha, beta))\n",
    "\n",
    "    def likelihood(self, X, Y, model, alpha, beta, gamma, w):\n",
    "        Pt = model(X, Y, alpha, beta, gamma, w)\n",
    "        return np.sum(Pt.T.dot(np.log(Pt)))\n",
    "    \n",
    "    def grad_likelihood(self, X, Y, model, alpha, beta, gamma, w):\n",
    "        \"\"\"Returns the partial derivatives of likelihood according to\n",
    "        alpha, beta, gamma and w\"\"\"\n",
    "        tmp_exp = np.exp(-X.dot(alpha.T)-beta)\n",
    "        zx = self.z_cond_x(X,alpha,beta)\n",
    "        deltaPt = zx[:,1]-zx[:,0]\n",
    "        deltaPt = deltaPt.reshape((deltaPt.shape[0],1))\n",
    "        grad_lh_alpha = np.sum(np.multiply(deltaPt,np.multiply(np.multiply(X,tmp_exp),1/(1+tmp_exp)**2)))\n",
    "        grad_lh_beta = np.sum(np.multiply(deltaPt,np.multiply(tmp_exp,1/(1+tmp_exp)**2)))\n",
    "        tmp_exp = np.exp(-X.dot(w)-gamma) # Taille : N,T\n",
    "        grad_etasigma_gamma = tmp_exp/(1+tmp_exp)**2 # Taille : N,T\n",
    "        print(tmp_exp.shape)\n",
    "        ############# INCOHERENT PRODUIT D'HADAMARD  ################ J'ai pas compris.\n",
    "        grad_etasigma_w = np.multiply(X,tmp_exp)/(1+tmp_exp)**2 # Taille : N,T\n",
    "        if (model==likelihoodBernoulli):\n",
    "            grad_lh_eta = (-1)**Y *(-deltaPt) # Taille : N,T\n",
    "            grad_lh_gamma = np.sum(np.multiply(grad_lh_eta,grad_etasigma_gamma)) \n",
    "            grad_lh_w = np.sum(np.multiply(grad_lh_eta, grad_etasigma,w))\n",
    "        elif (model==likelihoodGaussian):\n",
    "            s = self.rlog(np.arange(1,X.shape[0]+1),np.arange(1,Y.shape[1]+1),X,w,gamma)\n",
    "            grad_lh_sigma = (Y**2-self.Pz(1,X)*(2*Y-1))/s**3 - 1/s # Taille : N,T\n",
    "            grad_lh_gamma = np.sum(np.multiply(grad_lh_sigma,grad_etasigma_gamma)) \n",
    "            grad_lh_w = np.sum(np.multiply(grad_lh_sigma, grad_etasigma,w))\n",
    "        return np.array([[-grad_lh_alpha, -grad_lh_beta, -grad_lh_gamma, -grad_lh_w]])\n",
    "    \n",
    "    def fit(self, X, Y, model=likelihoodBernoulli, eps = 10**(-6)):\n",
    "        d = X.shape[1]\n",
    "        T = Y.shape[1]\n",
    "        self.alpha = np.zeros((1,d))\n",
    "        self.beta = 0\n",
    "        alphaNew = np.ones((1,d))\n",
    "        betaNew = 1\n",
    "        wNew = np.random.rand(d,T)\n",
    "        gammaNew = np.random.rand(1,T)\n",
    "        while (np.linalg.norm(self.alpha-alphaNew)**2 + (self.beta-betaNew)**2 >= eps):\n",
    "            tmpAlpha = alphaNew\n",
    "            tmpBeta = betaNew\n",
    "            tmpGamma = gammaNew\n",
    "            tmpW = wNew\n",
    "            # Expectation (E-step)\n",
    "            #Pt = model(self, X, Y, self.alpha, self.beta, self.gamma, self.w)\n",
    "            # Maximization\n",
    "            #lh = - self.likelihood(X, Y, model, alpha, beta, gamma, w)\n",
    "            BFGSfunc = lambda vect : self.likelihood(X, Y, model, vect[0], vect[1], vect[2], vect[3])\n",
    "            BFGSJac = lambda vect : self.grad_likelihood(X, Y, model, vect[0], vect[1], vect[2], vect[3])\n",
    "            result = minimize(BFGSfunc, np.array([self.alpha, self.beta, self.gamma, self.w]),\\\n",
    "                              method='BFGS', jac = BFGSJac, options={'gtol': 1e-6, 'disp': True, 'maxiter': 1000})\n",
    "            print(result.message)\n",
    "            print(\"Optimal solution :\")\n",
    "            print(result.x)\n",
    "            # Updating new vectors :\n",
    "            alphaNew = result.x[0]\n",
    "            betaNew = result.x[1]\n",
    "            gammaNew = result.x[2]\n",
    "            wNew = result.x[3]\n",
    "            self.alpha = tmpAlpha\n",
    "            self.beta = tmpBeta\n",
    "            self.gamma = gammaNew\n",
    "            self.w = wNew\n",
    "            \n",
    "        self.alpha = alphaNew\n",
    "        self.beta = betaNew\n",
    "        self.w = wNew\n",
    "        self.gamma = gammaNew\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return\n",
    "    def predictV2(self, Y):\n",
    "        return\n",
    "    def score(self, X, Z):\n",
    "        # On connaît la vérité terrain\n",
    "        return np.sum(predict(X)==Z)\n",
    "    \n",
    "    def get_eps(self):\n",
    "        return\n",
    "    def loss(self,data,y):\n",
    "        return\n",
    "    def loss_g(self,data,y):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,2) (4,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a008e76d8424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearnCrowd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_annote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-067019e24838>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, model, eps)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mBFGSfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mBFGSJac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBFGSfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBFGSJac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gtol'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal solution :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amine/miniconda3/lib/python3.5/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
      "\u001b[0;32m/home/amine/miniconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mgrad_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyfprime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m     \u001b[0mgfk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyfprime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amine/miniconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-067019e24838>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vect)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m#lh = - self.likelihood(X, Y, model, alpha, beta, gamma, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mBFGSfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mBFGSJac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mvect\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBFGSfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBFGSJac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gtol'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'disp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-067019e24838>\u001b[0m in \u001b[0;36mgrad_likelihood\u001b[0;34m(self, X, Y, model, alpha, beta, gamma, w)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mgrad_etasigma_gamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_exp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtmp_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# Taille : N,T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mgrad_etasigma_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtmp_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# Taille : N,T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Bernoulli\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mgrad_lh_eta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdeltaPt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Taille : N,T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,2) (4,3) "
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "T = 3\n",
    "d = 2\n",
    "\n",
    "S = LearnCrowd(T,N,d)\n",
    "S.fit(xtrain,y_annote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
