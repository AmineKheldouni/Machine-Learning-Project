{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning from the crowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Modules importés\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import *\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bloc génération de données:\n",
    "def gen_arti(centerx=1,centery=1,sigma=0.1,nbex=1000,data_type=0,epsilon=0.02):\n",
    "    \"\"\" Generateur de donnees,\n",
    "        :param centerx: centre des gaussiennes\n",
    "        :param centery:\n",
    "        :param sigma: des gaussiennes\n",
    "        :param nbex: nombre d'exemples\n",
    "        :param data_type: 0: melange 2 gaussiennes, 1: melange 4 gaussiennes, 2:echequier\n",
    "        :param epsilon: bruit dans les donnees\n",
    "        :return: data matrice 2d des donnnes,y etiquette des donnnees\n",
    "    \"\"\"\n",
    "    if data_type==0:\n",
    "         #melange de 2 gaussiennes\n",
    "         xpos=np.random.multivariate_normal([centerx,centerx],np.diag([sigma,sigma]),int(nbex//2))\n",
    "         xneg=np.random.multivariate_normal([-centerx,-centerx],np.diag([sigma,sigma]),int(nbex//2))\n",
    "         data=np.vstack((xpos,xneg))\n",
    "         y=np.hstack((np.ones(nbex//2),-np.ones(nbex//2)))\n",
    "    if data_type==1:\n",
    "        #melange de 4 gaussiennes\n",
    "        xpos=np.vstack((np.random.multivariate_normal([centerx,centerx],np.diag([sigma,sigma]),int(nbex//4)),np.random.multivariate_normal([-centerx,-centerx],np.diag([sigma,sigma]),int(nbex/4))))\n",
    "        xneg=np.vstack((np.random.multivariate_normal([-centerx,centerx],np.diag([sigma,sigma]),int(nbex//4)),np.random.multivariate_normal([centerx,-centerx],np.diag([sigma,sigma]),int(nbex/4))))\n",
    "        data=np.vstack((xpos,xneg))\n",
    "        y=np.hstack((np.ones(nbex//2),-np.ones(int(nbex//2))))\n",
    "\n",
    "    if data_type==2:\n",
    "        #echiquier\n",
    "        data=np.reshape(np.random.uniform(-4,4,2*nbex),(nbex,2))\n",
    "        y=np.ceil(data[:,0])+np.ceil(data[:,1])\n",
    "        y=2*(y % 2)-1\n",
    "    # un peu de bruit\n",
    "    data[:,0]+=np.random.normal(0,epsilon,nbex)\n",
    "    data[:,1]+=np.random.normal(0,epsilon,nbex)\n",
    "    # on mélange les données\n",
    "    idx = np.random.permutation((range(y.size)))\n",
    "    data=data[idx,:]\n",
    "    y=y[idx]\n",
    "    return data,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  1.  1.]\n",
      " [-1. -1. -1.]\n",
      " [-1.  1.  1.]\n",
      " [ 1.  1.  1.]]\n",
      "[-1. -1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#Fonction de générations de données \n",
    "\n",
    "#Proba que l'annotateur est raison\n",
    "\n",
    "liste_annotateur_bernouilli=[0.9,0.9,0.9]\n",
    "\n",
    "\n",
    "def fonction_modification_annotateur(label,proba):\n",
    "    valeur_proba=np.random.uniform(0,1)\n",
    "    label_res=label\n",
    "    if(valeur_proba>=proba):\n",
    "        label_res=-label\n",
    "        \n",
    "    return label_res\n",
    "        \n",
    "        \n",
    "\n",
    "def generation_bernouilli(nombre_exemple,datatype):\n",
    "    xtrain,ytrain = gen_arti(nbex=nombre_exemple,data_type=0,epsilon=0.2)\n",
    "    #changement des labels \n",
    "    y_annote=np.zeros((nombre_exemple,len(liste_annotateur_bernouilli)))\n",
    "    \n",
    "    for i in range(len(liste_annotateur_bernouilli)):\n",
    "        fonction_annotation=lambda x:fonction_modification_annotateur(x,liste_annotateur_bernouilli[i])\n",
    "        fonction_annotation=np.vectorize(fonction_annotation)\n",
    "        y_annote[:,i]=fonction_annotation(ytrain)\n",
    "    return xtrain,y_annote,ytrain\n",
    "\n",
    "        \n",
    "xtrain,y_annote,ytrain=generation_bernouilli(4,0)\n",
    "print(y_annote)\n",
    "print(ytrain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-2db244050362>, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-2db244050362>\"\u001b[0;36m, line \u001b[0;32m82\u001b[0m\n\u001b[0;31m    def predictV2(self, Y):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class LearnCrowd:\n",
    "    def __init__(self, T, N, d):\n",
    "        self.alpha = np.zeros((1,d)) # Poids des dimensions\n",
    "        self.beta = 0\n",
    "        self.w = np.zeros((d,T)) # Poids des labelleurs\n",
    "        self.gamma = np.zeros((1,T))\n",
    "        \n",
    "    def likelihoodBernouilli(self, X, Y):\n",
    "        #proba cond du label Yt du labelleur t pour la donnée i sachant le vrai label 0 ou 1 (Bernouilli)\n",
    "        y_cond_z = np.zeros((N,T,2))\n",
    "        rlog=lambda i,t: 1/(1+exp(-np.dot(self.w(:,t).T,X[i,:])-self.gamma[1,t]))   #indice de la donnée, indice du labelleur\n",
    "        rlog=np.vectorize(rlog) #rlog c'est neta\n",
    "        for (t in range(T)):\n",
    "            y_cond_z[:,t,0] = pow(1-rlog(:,t),np.abs(Y[:,t]))*pow(rlog(:,t),1-np.abs(Y[:,t]))\n",
    "            y_cond_z[:,t,1] = pow(1-rlog(:,t),np.abs(Y[:,t]-1))*pow(rlog(:,t),1-np.abs(Y[:,t]-1))\n",
    "        #hyp de base que l'on pourra prendre pour simplifier neta[i,t]=rlog(i,t)=neta[t]\n",
    "        #cet hyp revient à donner une proba constante de se tromper pour le labelleur t quelque soit la donnée\n",
    "        #il faudrait alors rajouter un self.neta=np.zeros(1,T) au init pour le modèle de Bernouilli\n",
    "        \n",
    "        #proba que le label soit 0 ou 1 sachant la donnée (Rlog)\n",
    "        z_cond_x = np.zeros((N,2))\n",
    "        sigm = lambda x:  1/(1+exp(-x))\n",
    "        sigm=np.vectorize(sigm)\n",
    "        z_cond_x[:,0] = sigm(np.dot(self.alpha.T,X)-self.beta)\n",
    "        z_cond_x[:,1] = 1-z_cond_x[:,0]\n",
    "        \n",
    "        #z_cond_x(i,1) = 1/(1+exp(-np.dot(self.alpha.T,X(i,:))-self.beta))\n",
    "        #z_cond_x(i,2) = 1-z_cond_x(i,1)\n",
    "        \n",
    "        return np.multiply(np.prod(y_cond_z,axis=1),z_cond_x)\n",
    "        \n",
    "    \n",
    "    def likelihoodGaussian(self, X, Y):\n",
    "        #proba cond du label Yt du labelleur t pour la donnée i sachant le vrai label 0 ou 1 (Bernouilli)\n",
    "        y_cond_z_cond_x = np.zeros((N,T,2))\n",
    "        rlog=lambda i,t: 1/(1+exp(-np.dot(self.w(:,t).T,X[i,:])-self.gamma[1,t]))   #indice de la donnée, indice du labelleur\n",
    "        rlog=np.vectorize(rlog)\n",
    "        norm=lambda x,mu,sigma:1/(sqrt(2*np.pi)*sigma)*exp(-pow((x-mu),2)/pow(sigma,2))\n",
    "        norm=np.vectorize(norm)\n",
    "        for (t in range(T)):\n",
    "            y_cond_z_cond_x[:,t,0] = norm(Y[:,t],0,rlog(:,t))\n",
    "            y_cond_z_cond_x[:,t,1] = norm(Y[:,t],1,rlog(:,t))\n",
    "        \n",
    "        #proba que le label soit 0 ou 1 sachant la donnée (Rlog)\n",
    "        z_cond_x = np.zeros((N,2))\n",
    "        sigm = lambda x:  1/(1+exp(-x))\n",
    "        sigm=np.vectorize(sigm)\n",
    "        z_cond_x[:,0] = sigm(np.dot(self.alpha.T,X)-self.beta)\n",
    "        z_cond_x[:,1] = 1-z_cond_x[:,0]\n",
    "        \n",
    "        return np.multiply(np.prod(y_cond_z_cond_x,axis=1),z_cond_x)\n",
    "        \n",
    "    def fit(self, X, Y, model=likelihoodBernouilli,  eps = 10**(-6)):\n",
    "          \n",
    "    def sigma(self, X):\n",
    "        # A FAIRE\n",
    "        return 0\n",
    "    def eta(self, X):\n",
    "        # A FAIRE\n",
    "        return 0\n",
    "    \n",
    "    def likelihoodBernoulli(self, X, Y, gamma, w):\n",
    "        P = np.zeros((X.shape[0],Y.shape[1]))\n",
    "        return P      \n",
    "\n",
    "    def likelihoodGaussian(self, X, Y, gamma, w):\n",
    "        P = np.zeros((X.shape[0],Y.shape[1]))\n",
    "        return P\n",
    "        \n",
    "    def Pz(self, z, X, alpha, beta):\n",
    "        res = 1/(1+np.exp(-alpha.dot(X.T)-beta))\n",
    "        if z == 1:\n",
    "            return res\n",
    "        else:\n",
    "            return 1-res\n",
    "        \n",
    "    def Ptilde(self, X, Y, model, alpha, beta, gamma, w):\n",
    "        Pt = np.zeros((X.shape[0],1))\n",
    "        py = self.model(X,Y, gamma, w) # Taille N,T\n",
    "        pz = self.Pz(1, X, alpha, beta) # Taille : N,1\n",
    "        return np.multiplty(np.prod(py,axis=1),pz) # Taille : N,1\n",
    "    \n",
    "    def likelihood(self, X, Y, model, alpha, beta, gamma, w):\n",
    "        Pt = self.Ptilde(X, Y, model, alpha, beta, gamma, w)\n",
    "        return Pt.T.dot(np.log(Pt))\n",
    "    \n",
    "    def grad_likelihood(self, X, Y, model, alpha, beta, gamma, w):\n",
    "        \"\"\"Returns the partial derivatives of likelihood according to\n",
    "        alpha, beta, gamma and w\"\"\"\n",
    "        tmp_exp = np.exp(-X.dot(alpha.T)-beta)\n",
    "        deltaPt = self.Pz(1,X)-self.Pz(0,X)\n",
    "        grad_lh_alpha = np.sum(deltaPt*np.multiplty(np.multiplty(X,tmp_exp),1/(1+tmp_exp)**2))\n",
    "        grad_lh_beta = np.sum(deltaPt*np.multiplty(tmp_exp,1/(1+tmp_exp)**2))\n",
    "        tmp_exp = np.exp(-X.dot(w)-gamma) # Taille : N,T\n",
    "        grad_etasigma_gamma = tmp_exp/(1+tmp_exp)**2 # Taille : N,T\n",
    "        grad_etasigma_w = np.multiplty(X,tmp_exp)/(1+tmp_exp)**2 # Taille : N,T\n",
    "        if (\"Bernoulli\" in model):\n",
    "            grad_lh_eta = (-1)**Y *(-deltaPt) # Taille : N,T\n",
    "            grad_lh_gamma = np.sum(np.multiply(grad_lh_eta,grad_etasigma_gamma)) \n",
    "            grad_lh_w = np.sum(np.multiply(grad_lh_eta, grad_etasigma,w))\n",
    "        elif (\"Gaussian\" in model):\n",
    "            s = self.sigma(X)\n",
    "            grad_lh_sigma = (Y**2-self.Pz(1,X)*(2*Y-1))/s**3 - 1/s # Taille : N,T\n",
    "            grad_lh_gamma = np.sum(np.multiply(grad_lh_sigma,grad_etasigma_gamma)) \n",
    "            grad_lh_w = np.sum(np.multiply(grad_lh_sigma, grad_etasigma,w))\n",
    "        return np.array([[-grad_lh_alpha, -grad_lh_beta, -grad_lh_gamma, -grad_lh_w]])\n",
    "    \n",
    "    def BFGS_func(self, X, Y, model, alpha, beta, gamma, w):\n",
    "        return [self.likelihood(X, Y, model, alpha, beta, gamma, w), \\\n",
    "                self.grad_likelihood(X, Y, model, alpha, beta, gamma, w)]\n",
    "    \n",
    "    def fit(self, X, Y, model=likelihoodBernoulli, eps = 10**(-6)):\n",
    "        self.alpha = np.zeros((1,d))\n",
    "        self.beta = 0\n",
    "        alphaNew = np.ones((1,d))\n",
    "        betaNew = 1\n",
    "        wNew = np.random.rand(d,T)\n",
    "        gammaNew = np.random.rand(1,T)\n",
    "        while (np.linalg.norm(self.alpha-alphaNew)**2 + (self.beta-betaNew)**2 >= eps):\n",
    "            tmpAlpha = alphaNew\n",
    "            tmpBeta = betaNew\n",
    "            tmpGamma = gammaNew\n",
    "            tmpW = wNew\n",
    "            # Expectation (E-step)\n",
    "            Pt = self.Ptilde(X, Y, Z, model)\n",
    "            # Maximization\n",
    "            lh = - self.likelihood(X, Y, model, alpha, beta, gamma, w)\n",
    "            BFGSfunc = lambda alpha,beta,gamma,w : likelihood(self, X, Y, model, alpha, beta, gamma, w)\n",
    "            BFGSJac = lambda alpha,beta,gamma,w : grad_likelihood(self, X, Y, model, alpha, beta, gamma, w)\n",
    "            result = minimize(BFGSfunc, method='BFGS', jac = BFGSJac, \\\n",
    "                              options={'gtol': 1e-6, 'disp': True, 'maxiter': 1000})\n",
    "            print(result.message)\n",
    "            print(\"Optimal solution :\")\n",
    "            print(result.x)\n",
    "            # Updating new vectors :\n",
    "            alphaNew = result.x[0]\n",
    "            betaNew = result.x[1]\n",
    "            gammaNew = result.x[2]\n",
    "            wNew = result.x[3]\n",
    "            self.alpha = tmpAlpha\n",
    "            self.beta = tmpBeta\n",
    "            self.gamma = gammaNew\n",
    "            self.w = wNew\n",
    "            \n",
    "        self.alpha = alphaNew\n",
    "        self.beta = betaNew\n",
    "        self.w = wNew\n",
    "        self.gamma = gammaNew\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "    def predictV2(self, Y):\n",
    "        \n",
    "    def score(self, X, Z):\n",
    "        # On connaît la vérité terrain\n",
    "        return np.sum(predict(X)==Z)\n",
    "    \n",
    "    def get_eps(self):\n",
    "        \n",
    "    def loss(self,data,y):\n",
    "        \n",
    "    def loss_g(self,data,y):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
